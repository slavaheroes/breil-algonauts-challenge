GRADIENT_ACCUMULATION_STEPS: 1
GRADIENT_CLIPPING: Null
EPOCHS: 200
CHECK_VAL_EPOCHS: 2

BATCH_SIZE: 16
NUM_WORKERS: 8

LOAD_PRETRAINED:
  ENABLE: False
  ARGS:
    checkpoint_path: ""
  
OPTIMIZER:
  MODULE: torch.optim
  CLASS: AdamW
  ARGS:
    lr: 0.00003
    weight_decay: 0.2
    betas: [0.1, 0.9]

SCHEDULER:
  MODULE: torch.optim.lr_scheduler
  CLASS: CosineAnnealingLR
  ARGS:
    T_max: 100
    eta_min: 0.0000001

EARLY_STOPPING:
  ENABLE: True
  ARGS:
    monitor: valid_loss
    mode: min
    patience: 20
    verbose: True

LR_MONITOR:
  ENABLE: True
  ARGS:
    logging_interval: 'epoch'
    log_momentum: False

CHECKPOINT:
  ARGS:
    save_top_k: 1
    verbose: True
    monitor: valid_loss
    mode: min